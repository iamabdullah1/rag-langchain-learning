{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0050d41e",
   "metadata": {},
   "source": [
    "## ğŸ¤” Why Do We Need Chunking?\n",
    "\n",
    "**Problem:** AI models have limited \"context windows\"\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Your PDF: 50 pages Ã— 3000 chars = 150,000 characters   â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚ AI Context Window: ~4,000 - 8,000 characters           â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚ Problem: Can't fit entire document! âŒ                  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Solution:** Split into small chunks, find the relevant ones, send only those!\n",
    "\n",
    "```\n",
    "Full Document  â†’  Split into Chunks  â†’  Find Relevant  â†’  Send to AI\n",
    "(too big)          (500 chars each)      (2-3 chunks)     (fits!)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8085ae65",
   "metadata": {},
   "source": [
    "## Step 1: Load Documents (from Notebook 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe4b5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's load our documents\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import os\n",
    "\n",
    "# Set up paths\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "data_folder = os.path.join(project_root, 'data')\n",
    "\n",
    "# Load all PDFs\n",
    "all_pages = []\n",
    "pdf_files = [f for f in os.listdir(data_folder) if f.endswith('.pdf')]\n",
    "\n",
    "print(\"ğŸ“š Loading PDFs...\")\n",
    "for pdf_name in pdf_files:\n",
    "    loader = PyPDFLoader(os.path.join(data_folder, pdf_name))\n",
    "    pages = loader.load()\n",
    "    all_pages.extend(pages)\n",
    "    print(f\"   âœ… {pdf_name}: {len(pages)} pages\")\n",
    "\n",
    "print(f\"\\nğŸ“„ Total pages: {len(all_pages)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9912e24",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Understand the Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f447398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how big our pages are\n",
    "print(\"ğŸ“Š Page Size Analysis:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "page_lengths = [len(page.page_content) for page in all_pages]\n",
    "\n",
    "print(f\"Shortest page: {min(page_lengths):,} characters\")\n",
    "print(f\"Longest page:  {max(page_lengths):,} characters\")\n",
    "print(f\"Average page:  {sum(page_lengths)//len(page_lengths):,} characters\")\n",
    "print(f\"Total content: {sum(page_lengths):,} characters\")\n",
    "\n",
    "print(\"\\nâš ï¸ Problem: Most AI models can only handle 4,000-8,000 characters at once!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d41892",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Introduction to Text Splitting\n",
    "\n",
    "LangChain provides several text splitters:\n",
    "\n",
    "| Splitter | Description | Best For |\n",
    "|----------|-------------|----------|\n",
    "| CharacterTextSplitter | Splits by character count | Simple cases |\n",
    "| **RecursiveCharacterTextSplitter** | Splits smartly at sentence/paragraph boundaries | **Most cases** âœ“ |\n",
    "| TokenTextSplitter | Splits by token count | When using specific models |\n",
    "| MarkdownTextSplitter | Respects markdown structure | Markdown documents |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d964090a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the recommended text splitter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "print(\"âœ… RecursiveCharacterTextSplitter imported!\")\n",
    "print(\"\\nğŸ“– Why 'Recursive'?\")\n",
    "print(\"   It tries to split at natural boundaries:\")\n",
    "print(\"   1. First, try splitting at paragraphs (\\\\n\\\\n)\")\n",
    "print(\"   2. If still too big, try sentences (.)\")\n",
    "print(\"   3. If still too big, try words (space)\")\n",
    "print(\"   4. Last resort: split at characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc46dcf7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Configure the Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2810c59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text splitter with specific settings\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,      # Target size of each chunk\n",
    "    chunk_overlap=50,    # Overlap between chunks\n",
    "    length_function=len, # How to measure length\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # Split priorities\n",
    ")\n",
    "\n",
    "print(\"âœ… Text splitter configured!\")\n",
    "print(f\"\\nğŸ“ Settings:\")\n",
    "print(f\"   Chunk size: 500 characters\")\n",
    "print(f\"   Overlap: 50 characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c35e5ee",
   "metadata": {},
   "source": [
    "### ğŸ’¡ Understanding the Parameters\n",
    "\n",
    "**chunk_size** (500 characters)\n",
    "- How big each chunk should be\n",
    "- Smaller = more precise retrieval, but less context\n",
    "- Larger = more context, but might include irrelevant info\n",
    "\n",
    "**chunk_overlap** (50 characters)\n",
    "- How much chunks should overlap\n",
    "- Prevents cutting sentences in half\n",
    "\n",
    "```\n",
    "Chunk 1: [........text content.......][overlap]\n",
    "Chunk 2:                       [overlap][........text content.......]\n",
    "                                   â†‘\n",
    "                           Shared content!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62f1b43",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Split the Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55511a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split all pages into chunks\n",
    "print(\"âœ‚ï¸ Splitting documents into chunks...\\n\")\n",
    "\n",
    "chunks = text_splitter.split_documents(all_pages)\n",
    "\n",
    "print(f\"ğŸ“„ Original pages: {len(all_pages)}\")\n",
    "print(f\"ğŸ“¦ Chunks created: {len(chunks)}\")\n",
    "print(f\"\\nğŸ“Š Ratio: {len(chunks)/len(all_pages):.1f} chunks per page (average)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb7913a",
   "metadata": {},
   "source": [
    "### ğŸ’¡ Explanation\n",
    "\n",
    "```\n",
    "Before:  25 pages  (each ~2000-3000 characters)\n",
    "                    â†“\n",
    "After:   ~100 chunks  (each ~500 characters)\n",
    "```\n",
    "\n",
    "Each page gets split into multiple smaller chunks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39036fe7",
   "metadata": {},
   "source": [
    "## Step 6: Examine the Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c93af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at chunk statistics\n",
    "print(\"ğŸ“Š Chunk Size Analysis:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "chunk_lengths = [len(chunk.page_content) for chunk in chunks]\n",
    "\n",
    "print(f\"Smallest chunk: {min(chunk_lengths)} characters\")\n",
    "print(f\"Largest chunk:  {max(chunk_lengths)} characters\")\n",
    "print(f\"Average chunk:  {sum(chunk_lengths)//len(chunk_lengths)} characters\")\n",
    "\n",
    "# Show distribution\n",
    "print(\"\\nğŸ“ˆ Size Distribution:\")\n",
    "ranges = [(0, 200), (200, 400), (400, 500), (500, 600)]\n",
    "for low, high in ranges:\n",
    "    count = sum(1 for l in chunk_lengths if low <= l < high)\n",
    "    bar = \"â–ˆ\" * (count // 2)\n",
    "    print(f\"   {low:3}-{high:3}: {bar} ({count})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f567ebf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at a sample chunk\n",
    "print(\"ğŸ“ Sample Chunk (first one):\")\n",
    "print(\"=\"*60)\n",
    "print(chunks[0].page_content)\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nğŸ“‹ Metadata: {chunks[0].metadata}\")\n",
    "print(f\"ğŸ“ Length: {len(chunks[0].page_content)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dda667f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Visualize the Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8987ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find consecutive chunks from the same page to show overlap\n",
    "print(\"ğŸ” Finding overlap between consecutive chunks...\\n\")\n",
    "\n",
    "# Get chunks from the same page\n",
    "page_1_chunks = [c for c in chunks if c.metadata.get('page', -1) == 1]\n",
    "\n",
    "if len(page_1_chunks) >= 2:\n",
    "    chunk_a = page_1_chunks[0]\n",
    "    chunk_b = page_1_chunks[1]\n",
    "    \n",
    "    # Show end of first chunk\n",
    "    print(\"ğŸ“„ End of Chunk 1:\")\n",
    "    print(\"-\"*40)\n",
    "    print(f\"...{chunk_a.page_content[-100:]}\")\n",
    "    \n",
    "    # Show start of second chunk\n",
    "    print(\"\\nğŸ“„ Start of Chunk 2:\")\n",
    "    print(\"-\"*40)\n",
    "    print(f\"{chunk_b.page_content[:100]}...\")\n",
    "    \n",
    "    # Find the overlap\n",
    "    end_text = chunk_a.page_content[-50:]\n",
    "    if end_text in chunk_b.page_content:\n",
    "        print(\"\\nâœ… Overlap detected! The end of Chunk 1 appears at the start of Chunk 2.\")\n",
    "else:\n",
    "    print(\"Not enough chunks from same page to demonstrate overlap.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1448d4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ§ª Experiment: Different Chunk Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d14aed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different chunk sizes\n",
    "print(\"ğŸ§ª Experimenting with different chunk sizes:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "sizes_to_try = [200, 500, 1000, 2000]\n",
    "\n",
    "for size in sizes_to_try:\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=size,\n",
    "        chunk_overlap=size // 10  # 10% overlap\n",
    "    )\n",
    "    test_chunks = splitter.split_documents(all_pages)\n",
    "    \n",
    "    print(f\"\\n   Chunk size {size}:\")\n",
    "    print(f\"   â””â”€â”€ {len(test_chunks)} chunks created\")\n",
    "    print(f\"       (avg: {sum(len(c.page_content) for c in test_chunks)//len(test_chunks)} chars)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2201ed9c",
   "metadata": {},
   "source": [
    "### ğŸ’¡ Choosing Chunk Size\n",
    "\n",
    "| Size | Chunks | Pros | Cons |\n",
    "|------|--------|------|------|\n",
    "| **200** | Many | Very precise retrieval | Less context per chunk |\n",
    "| **500** | Medium | Good balance âœ“ | Good for most cases |\n",
    "| **1000** | Fewer | More context | Might include irrelevant text |\n",
    "| **2000** | Few | Lots of context | Harder to find specific info |\n",
    "\n",
    "**Recommendation:** Start with 500 and adjust based on your results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554b672c",
   "metadata": {},
   "source": [
    "## âœ… Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **Why chunking is needed** - AI models have limited context windows\n",
    "2. **RecursiveCharacterTextSplitter** - Splits at natural boundaries\n",
    "3. **Chunk size** - Balance between precision and context (~500 chars)\n",
    "4. **Chunk overlap** - Prevents cutting sentences in half (~50 chars)\n",
    "\n",
    "## â¡ï¸ Next Step\n",
    "\n",
    "In **Notebook 3: Embeddings**, you'll learn how to convert these text chunks into numbers (vectors) that computers can search.\n",
    "\n",
    "---\n",
    "\n",
    "**Variables to use in next notebook:**\n",
    "- `chunks` - List of Document chunks ready for embedding"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9288628b",
   "metadata": {},
   "source": [
    "## ğŸ¨ The Complete Picture\n",
    "\n",
    "Here's what we've built across all notebooks:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    RAG PIPELINE                             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                             â”‚\n",
    "â”‚   ğŸ“„ PDFs â†’ âœ‚ï¸ Chunks â†’ ğŸ”¢ Embeddings â†’ ğŸ—„ï¸ ChromaDB        â”‚\n",
    "â”‚   (Nb 1)    (Nb 2)      (Nb 3)          (Nb 4)             â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚   â”‚              THIS NOTEBOOK (Nb 5)                   â”‚  â”‚\n",
    "â”‚   â”‚                                                     â”‚  â”‚\n",
    "â”‚   â”‚   Question â†’ Search â†’ Context â†’ AI â†’ Answer        â”‚  â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â”‚                                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4820f83c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Set Up Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225130cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete setup - all previous steps in one cell\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Set up paths\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "data_folder = os.path.join(project_root, 'data')\n",
    "chroma_path = os.path.join(os.getcwd(), 'chroma_db')\n",
    "\n",
    "print(\"ğŸš€ Setting up RAG Pipeline...\\n\")\n",
    "\n",
    "# Step 1: Load PDFs\n",
    "print(\"ğŸ“„ Step 1: Loading PDFs...\")\n",
    "all_pages = []\n",
    "for pdf_name in os.listdir(data_folder):\n",
    "    if pdf_name.endswith('.pdf'):\n",
    "        loader = PyPDFLoader(os.path.join(data_folder, pdf_name))\n",
    "        all_pages.extend(loader.load())\n",
    "print(f\"   âœ… Loaded {len(all_pages)} pages\")\n",
    "\n",
    "# Step 2: Create chunks\n",
    "print(\"âœ‚ï¸ Step 2: Creating chunks...\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(all_pages)\n",
    "print(f\"   âœ… Created {len(chunks)} chunks\")\n",
    "\n",
    "# Step 3: Load embeddings\n",
    "print(\"ğŸ”¢ Step 3: Loading embeddings model...\")\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'}\n",
    ")\n",
    "print(\"   âœ… Embeddings ready\")\n",
    "\n",
    "# Step 4: Create vector store\n",
    "print(\"ğŸ—„ï¸ Step 4: Creating ChromaDB...\")\n",
    "if os.path.exists(chroma_path):\n",
    "    shutil.rmtree(chroma_path)\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=chroma_path\n",
    ")\n",
    "print(f\"   âœ… Vector store ready ({len(chunks)} chunks)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ‰ Pipeline ready! Now let's connect the AI...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65261d43",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Connect to Groq API (AI Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4f5df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Groq - a fast, free AI API\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API key from .env file\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "if not api_key or api_key == \"your_groq_api_key_here\":\n",
    "    print(\"âš ï¸ Please set up your Groq API key!\")\n",
    "    print(\"\\nğŸ“ Steps:\")\n",
    "    print(\"   1. Go to: https://console.groq.com/keys\")\n",
    "    print(\"   2. Create a free account\")\n",
    "    print(\"   3. Generate an API key\")\n",
    "    print(\"   4. Create a .env file in the project root with:\")\n",
    "    print(\"      GROQ_API_KEY=your_actual_key_here\")\n",
    "    print(\"   5. Run this cell again\")\n",
    "else:\n",
    "    print(\"ğŸ¤– Connecting to Groq API...\")\n",
    "    \n",
    "    llm = ChatGroq(\n",
    "        model=\"llama-3.1-8b-instant\",\n",
    "        temperature=0.3,\n",
    "        api_key=api_key\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… Connected!\")\n",
    "    print(\"   Model: llama-3.1-8b-instant\")\n",
    "    print(\"   Speed: ~2-5 seconds per response! ğŸš€\")\n",
    "    \n",
    "    # Quick test\n",
    "    print(\"\\nâ³ Testing connection...\")\n",
    "    response = llm.invoke(\"Say 'RAG is ready!' in 4 words.\")\n",
    "    print(f\"âœ… AI says: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58768ff",
   "metadata": {},
   "source": [
    "### ğŸ’¡ About Groq\n",
    "\n",
    "**Groq** provides extremely fast AI inference:\n",
    "\n",
    "| Provider | Speed | Cost |\n",
    "|----------|-------|------|\n",
    "| Local Ollama | 60-120 sec | Free |\n",
    "| OpenAI | 3-8 sec | ~$0.002/query |\n",
    "| **Groq** | **2-5 sec** | **Free tier!** |\n",
    "\n",
    "We use `llama-3.1-8b-instant` - a fast, capable model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cbe38c",
   "metadata": {},
   "source": [
    "## Step 3: Create the RAG Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03db1e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_my_documents(question, num_chunks=3, show_sources=True):\n",
    "    \"\"\"\n",
    "    Ask a question about your PDF documents using RAG.\n",
    "    \n",
    "    Args:\n",
    "        question: Your question as a string\n",
    "        num_chunks: How many document chunks to retrieve (default: 3)\n",
    "        show_sources: Whether to print source information (default: True)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with 'answer' and 'sources'\n",
    "    \"\"\"\n",
    "    \n",
    "    # ========================================\n",
    "    # STEP A: RETRIEVAL - Find relevant chunks\n",
    "    # ========================================\n",
    "    docs = vectorstore.similarity_search(question, k=num_chunks)\n",
    "    \n",
    "    # ========================================\n",
    "    # STEP B: Build context from chunks\n",
    "    # ========================================\n",
    "    context = \"\"\n",
    "    sources = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        context += doc.page_content + \"\\n\\n\"\n",
    "        source_info = {\n",
    "            'file': doc.metadata.get('source', 'Unknown').split('/')[-1],\n",
    "            'page': doc.metadata.get('page', '?')\n",
    "        }\n",
    "        sources.append(source_info)\n",
    "    \n",
    "    # ========================================\n",
    "    # STEP C: Create prompt for AI\n",
    "    # ========================================\n",
    "    prompt = f\"\"\"You are a helpful research assistant. Answer the question based ONLY on the provided context.\n",
    "If the answer is not in the context, say \"I don't have enough information to answer that based on the provided documents.\"\n",
    "\n",
    "Context from documents:\n",
    "---\n",
    "{context}\n",
    "---\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # ========================================\n",
    "    # STEP D: GENERATION - Get AI response\n",
    "    # ========================================\n",
    "    response = llm.invoke(prompt)\n",
    "    answer = response.content if hasattr(response, 'content') else str(response)\n",
    "    \n",
    "    # ========================================\n",
    "    # STEP E: Return results\n",
    "    # ========================================\n",
    "    return {\n",
    "        'answer': answer.strip(),\n",
    "        'sources': sources,\n",
    "        'context': context  # Included for transparency\n",
    "    }\n",
    "\n",
    "print(\"âœ… RAG function created!\")\n",
    "print(\"\\nğŸ“– Usage:\")\n",
    "print('   result = ask_my_documents(\"Your question here\")')\n",
    "print('   print(result[\"answer\"])')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a34726a",
   "metadata": {},
   "source": [
    "### ğŸ’¡ Understanding the RAG Function\n",
    "\n",
    "```\n",
    "Question: \"What is qualitative coding?\"\n",
    "              â”‚\n",
    "              â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ STEP A: RETRIEVAL                   â”‚\n",
    "â”‚ Search ChromaDB for similar chunks  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "              â”‚\n",
    "              â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ STEP B: BUILD CONTEXT               â”‚\n",
    "â”‚ Combine relevant chunks into text   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "              â”‚\n",
    "              â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ STEP C: CREATE PROMPT               â”‚\n",
    "â”‚ \"Based on this context, answer...\"  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "              â”‚\n",
    "              â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ STEP D: GENERATION                  â”‚\n",
    "â”‚ Send to AI (Groq) â†’ Get answer      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "              â”‚\n",
    "              â–¼\n",
    "        Answer + Sources\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3268746",
   "metadata": {},
   "source": [
    "## Step 4: Ask Your First Question! ğŸ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24431eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Your question\n",
    "my_question = \"What is qualitative data coding and why is it important?\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"â“ Question: {my_question}\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nâ³ Processing...\\n\")\n",
    "\n",
    "# Time the response\n",
    "start = time.time()\n",
    "result = ask_my_documents(my_question)\n",
    "duration = time.time() - start\n",
    "\n",
    "# Display the answer\n",
    "print(\"ğŸ’¬ ANSWER:\")\n",
    "print(\"-\"*60)\n",
    "print(result['answer'])\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Show sources\n",
    "print(\"\\nğŸ“š Sources used:\")\n",
    "for i, src in enumerate(result['sources'], 1):\n",
    "    print(f\"   {i}. {src['file']}, Page {src['page']}\")\n",
    "\n",
    "print(f\"\\nâ±ï¸ Response time: {duration:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dff89f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Try More Questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e446047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for clean output\n",
    "def ask(question):\n",
    "    print(f\"\\nâ“ {question}\")\n",
    "    print(\"-\"*50)\n",
    "    result = ask_my_documents(question)\n",
    "    print(f\"ğŸ’¬ {result['answer'][:500]}...\" if len(result['answer']) > 500 else f\"ğŸ’¬ {result['answer']}\")\n",
    "    return result\n",
    "\n",
    "# Try different questions\n",
    "ask(\"What are the challenges in qualitative research?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1388db",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask(\"How do you analyze interview data?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e295bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask(\"What is thematic analysis?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fa4e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with an irrelevant question\n",
    "ask(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754d8288",
   "metadata": {},
   "source": [
    "### ğŸ’¡ Notice the Last Response!\n",
    "\n",
    "The AI correctly says it doesn't have information about France in the documents. This is RAG working correctly!\n",
    "\n",
    "- **Without RAG:** AI might hallucinate an answer based on its training\n",
    "- **With RAG:** AI only answers based on YOUR documents\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ca3ada",
   "metadata": {},
   "source": [
    "## Step 6: View the Retrieved Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205fb7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See exactly what context was sent to the AI\n",
    "question = \"What is coding in qualitative research?\"\n",
    "result = ask_my_documents(question)\n",
    "\n",
    "print(\"ğŸ” Context sent to AI:\")\n",
    "print(\"=\"*60)\n",
    "print(result['context'])\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nğŸ’¬ Answer:\")\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555bba3f",
   "metadata": {},
   "source": [
    "### ğŸ’¡ Transparency in RAG\n",
    "\n",
    "One benefit of RAG is **transparency**:\n",
    "- You can see exactly what context was used\n",
    "- You know which documents contained the answer\n",
    "- You can verify the AI's response against the sources\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de1a566",
   "metadata": {},
   "source": [
    "## ğŸ§ª Your Turn: Ask Your Own Questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528e2d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœï¸ Try your own question here!\n",
    "my_question = \"YOUR QUESTION HERE\"\n",
    "\n",
    "result = ask_my_documents(my_question)\n",
    "print(f\"â“ {my_question}\\n\")\n",
    "print(f\"ğŸ’¬ {result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590529c1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ Congratulations! You Built a RAG System!\n",
    "\n",
    "### What You Accomplished:\n",
    "\n",
    "1. **ğŸ“„ Document Loading** - Read PDFs into Python\n",
    "2. **âœ‚ï¸ Text Chunking** - Split into searchable pieces\n",
    "3. **ğŸ”¢ Embeddings** - Convert text to vectors\n",
    "4. **ğŸ—„ï¸ Vector Store** - Store in ChromaDB\n",
    "5. **ğŸ¤– AI Integration** - Connect to Groq API\n",
    "6. **â“ Question Answering** - Full RAG pipeline!\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- **RAG = Retrieval + Generation**\n",
    "- **Vector search** finds relevant content\n",
    "- **AI generates** answers from that content\n",
    "- **Sources are traceable** - you know where answers come from\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Add your own PDFs to the `data/` folder\n",
    "- Experiment with different chunk sizes\n",
    "- Try different AI models\n",
    "- Build a web interface with Streamlit or Gradio\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Building! ğŸš€**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

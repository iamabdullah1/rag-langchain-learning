{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db4d15ae",
   "metadata": {},
   "source": [
    "## What is Chunking?\n",
    "\n",
    "**Chunking** is the process of breaking large documents into smaller pieces (chunks).\n",
    "\n",
    "### Why Do We Need Chunking?\n",
    "\n",
    "**Problem:** You have a 50-page research paper, but:\n",
    "1. **LLMs have token limits** - Can't process entire paper at once (e.g., GPT-3.5 = 4K tokens)\n",
    "2. **Better retrieval** - When searching, you want specific relevant sections, not the whole document\n",
    "3. **More precise answers** - Smaller chunks = more focused context for the LLM\n",
    "4. **Cost efficiency** - Only send relevant chunks to the LLM, not the entire document\n",
    "\n",
    "### Real-World Analogy:\n",
    "\n",
    "Imagine you're studying for an exam:\n",
    "- ‚ùå **No chunking:** Reading the entire 500-page textbook every time you have a question\n",
    "- ‚úÖ **With chunking:** Having an index that points you to specific pages/paragraphs relevant to your question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5702550b",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d21bba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install langchain langchain-community langchain-text-splitters pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34e01e0",
   "metadata": {},
   "source": [
    "## Load Your Research Papers\n",
    "\n",
    "Let's start by loading the two research papers from the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0944a366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Paper 1: 27 pages\n",
      "üìÑ Paper 2: 11 pages\n",
      "\n",
      "Total pages: 38\n",
      "\n",
      "==================================================\n",
      "First page preview:\n",
      "==================================================\n",
      "1 \n",
      "Coding qualitative data: a synthesis to guide the novice \n",
      " \n",
      "Mai Skj√∏tt Linneberg \n",
      "Department of Management, Aarhus University \n",
      " \n",
      "Steffen Korsgaard \n",
      "Department of Entrepreneurship and Relationship Management \n",
      " \n",
      " \n",
      " \n",
      "This is a pre-print version of our paper that will be published in Qualitative Research \n",
      "Journal (https://doi.org/10.1108/QRJ -12-2018-0012)...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import os\n",
    "\n",
    "# Get the project root directory\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "data_dir = os.path.join(project_root, 'data')\n",
    "\n",
    "# Load both research papers\n",
    "pdf1_path = os.path.join(data_dir, 'CodingqualitativedataResearchgate.pdf')\n",
    "pdf2_path = os.path.join(data_dir, 'EJ1172284.pdf')\n",
    "\n",
    "# Load first paper\n",
    "loader1 = PyPDFLoader(pdf1_path)\n",
    "pages1 = loader1.load()\n",
    "\n",
    "# Load second paper\n",
    "loader2 = PyPDFLoader(pdf2_path)\n",
    "pages2 = loader2.load()\n",
    "\n",
    "print(f\"üìÑ Paper 1: {len(pages1)} pages\")\n",
    "print(f\"üìÑ Paper 2: {len(pages2)} pages\")\n",
    "print(f\"\\nTotal pages: {len(pages1) + len(pages2)}\")\n",
    "\n",
    "# Let's look at the first page\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"First page preview:\")\n",
    "print(f\"{'='*50}\")\n",
    "print(pages1[0].page_content[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e30a21d",
   "metadata": {},
   "source": [
    "## Understanding Chunking Parameters\n",
    "\n",
    "Before we chunk, let's understand the key parameters:\n",
    "\n",
    "### 1. **chunk_size** (Most Important)\n",
    "- The maximum number of characters in each chunk\n",
    "- Too small ‚Üí Loses context, many chunks\n",
    "- Too large ‚Üí Still hits token limits, less precise\n",
    "- **Sweet spot:** 500-1500 characters (for most use cases)\n",
    "\n",
    "### 2. **chunk_overlap**\n",
    "- How many characters to share between consecutive chunks\n",
    "- Prevents information from being cut off mid-sentence\n",
    "- **Typical:** 10-20% of chunk_size\n",
    "\n",
    "### 3. **separators**\n",
    "- How to split text (paragraphs, sentences, etc.)\n",
    "- Default: `[\"\\n\\n\", \"\\n\", \" \", \"\"]`\n",
    "\n",
    "### Visual Example:\n",
    "\n",
    "```\n",
    "Original Document: [------ 3000 characters ------]\n",
    "\n",
    "With chunk_size=1000, chunk_overlap=200:\n",
    "\n",
    "Chunk 1: [------ 1000 chars ------]\n",
    "Chunk 2:         [overlap][------ 1000 chars ------]\n",
    "Chunk 3:                          [overlap][------ 1000 chars ------]\n",
    "\n",
    "The overlap ensures important context isn't lost at boundaries!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6266a68f",
   "metadata": {},
   "source": [
    "## Method 1: RecursiveCharacterTextSplitter ‚≠ê Most Popular\n",
    "\n",
    "This is the **most commonly used** splitter. It tries to keep paragraphs, sentences, and words together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7abc9fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 27 pages\n",
      "After chunking: 67 chunks\n",
      "\n",
      "Average chunk size: 793 characters\n",
      "\n",
      "============================================================\n",
      "First 3 chunks:\n",
      "============================================================\n",
      "\n",
      "--- Chunk 1 (357 chars) ---\n",
      "1 \n",
      "Coding qualitative data: a synthesis to guide the novice \n",
      " \n",
      "Mai Skj√∏tt Linneberg \n",
      "Department of Management, Aarhus University \n",
      " \n",
      "Steffen Korsgaard \n",
      "Department of Entrepreneurship and Relationship M...\n",
      "Metadata: {'producer': 'Microsoft¬Æ Word for Office 365', 'creator': 'Microsoft¬Æ Word for Office 365', 'creationdate': '2019-05-16T11:07:21+02:00', 'author': 'Steffen Korsgaard', 'moddate': '2019-05-16T11:07:21+02:00', 'source': '/Users/apple/Desktop/RAG Langchain/data/CodingqualitativedataResearchgate.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1'}\n",
      "\n",
      "--- Chunk 2 (933 chars) ---\n",
      "2 \n",
      "Purpose \n",
      "Qualitative research has gained in importance in the social sciences. General knowledge about \n",
      "qualitative data analysis, how to code qualitative data and decisions concerning related \n",
      "res...\n",
      "Metadata: {'producer': 'Microsoft¬Æ Word for Office 365', 'creator': 'Microsoft¬Æ Word for Office 365', 'creationdate': '2019-05-16T11:07:21+02:00', 'author': 'Steffen Korsgaard', 'moddate': '2019-05-16T11:07:21+02:00', 'source': '/Users/apple/Desktop/RAG Langchain/data/CodingqualitativedataResearchgate.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2'}\n",
      "\n",
      "--- Chunk 3 (884 chars) ---\n",
      "craft of coding. We thus discuss the central choices that have to be made before, during and \n",
      "after coding, providing support for novices in practicing careful and enlightening coding work, \n",
      "and joini...\n",
      "Metadata: {'producer': 'Microsoft¬Æ Word for Office 365', 'creator': 'Microsoft¬Æ Word for Office 365', 'creationdate': '2019-05-16T11:07:21+02:00', 'author': 'Steffen Korsgaard', 'moddate': '2019-05-16T11:07:21+02:00', 'source': '/Users/apple/Desktop/RAG Langchain/data/CodingqualitativedataResearchgate.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Create the splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,        # Maximum characters per chunk\n",
    "    chunk_overlap=200,      # Overlap between chunks\n",
    "    length_function=len,    # How to measure chunk size\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Try to split on paragraphs first, then sentences, then words\n",
    ")\n",
    "\n",
    "# Split the first research paper\n",
    "chunks = text_splitter.split_documents(pages1)\n",
    "\n",
    "print(f\"Original: {len(pages1)} pages\")\n",
    "print(f\"After chunking: {len(chunks)} chunks\")\n",
    "print(f\"\\nAverage chunk size: {sum(len(chunk.page_content) for chunk in chunks) / len(chunks):.0f} characters\")\n",
    "\n",
    "# Look at first 3 chunks\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"First 3 chunks:\")\n",
    "print(f\"{'='*60}\")\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"\\n--- Chunk {i+1} ({len(chunk.page_content)} chars) ---\")\n",
    "    print(chunk.page_content[:200] + \"...\")\n",
    "    print(f\"Metadata: {chunk.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7b7f02",
   "metadata": {},
   "source": [
    "## Method 2: CharacterTextSplitter (Simple)\n",
    "\n",
    "Simpler than Recursive - splits on a single separator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87a2da03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple splitting created: 11 chunks\n",
      "First chunk preview:\n",
      "The EUROCALL Review, Volume 25, No. 2, September 2017 \n",
      " \n",
      " 18 \n",
      "Research paper \n",
      " \n",
      "A look at advanced learners‚Äô use of mobile devices for \n",
      "English language study: Insights from interview data \n",
      "Mariusz Kruk \n",
      "University of Zielona Gora, Poland \n",
      "____________________________________________________________...\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# Simple splitter - splits on double newlines (paragraphs)\n",
    "simple_splitter = CharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    separator=\"\\n\\n\"  # Only split on paragraph breaks\n",
    ")\n",
    "\n",
    "simple_chunks = simple_splitter.split_documents(pages2)\n",
    "\n",
    "print(f\"Simple splitting created: {len(simple_chunks)} chunks\")\n",
    "print(f\"First chunk preview:\")\n",
    "print(simple_chunks[0].page_content[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d3d3bf",
   "metadata": {},
   "source": [
    "## Method 3: TokenTextSplitter (For LLMs)\n",
    "\n",
    "Splits based on **tokens** (what LLMs actually count), not characters.\n",
    "\n",
    "**Why use tokens?**\n",
    "- LLMs count in tokens, not characters\n",
    "- 1 token ‚âà 4 characters (English)\n",
    "- More accurate for staying within LLM limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c99f7fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token-based splitting created: 9 chunks\n",
      "\n",
      "First chunk:\n",
      "1 \n",
      "Coding qualitative data: a synthesis to guide the novice \n",
      " \n",
      "Mai Skj√∏tt Linneberg \n",
      "Department of Management, Aarhus University \n",
      " \n",
      "Steffen Korsgaard \n",
      "Department of Entrepreneurship and Relationship Management \n",
      " \n",
      " \n",
      " \n",
      "This is a pre-print version of our paper that will be published in Qualitative Rese...\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "# Split by tokens (more accurate for LLMs)\n",
    "token_splitter = TokenTextSplitter(\n",
    "    chunk_size=250,         # 250 tokens (roughly 1000 characters)\n",
    "    chunk_overlap=50        # 50 tokens overlap\n",
    ")\n",
    "\n",
    "token_chunks = token_splitter.split_documents(pages1[:5])  # Use first 5 pages\n",
    "\n",
    "print(f\"Token-based splitting created: {len(token_chunks)} chunks\")\n",
    "print(f\"\\nFirst chunk:\")\n",
    "print(token_chunks[0].page_content[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003c84c8",
   "metadata": {},
   "source": [
    "## Method 4: Semantic Chunking (Advanced) üöÄ\n",
    "\n",
    "**Most intelligent** - splits based on meaning, not just character count!\n",
    "\n",
    "Uses embeddings to understand where topics change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570bebe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Semantic chunking requires embeddings (OpenAI API or local models)\n",
    "# For now, let's show the concept\n",
    "\n",
    "# from langchain_experimental.text_splitter import SemanticChunker\n",
    "# from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "# \n",
    "# semantic_splitter = SemanticChunker(\n",
    "#     OpenAIEmbeddings(),\n",
    "#     breakpoint_threshold_type=\"percentile\"  # Split when meaning changes significantly\n",
    "# )\n",
    "# \n",
    "# semantic_chunks = semantic_splitter.split_documents(pages1)\n",
    "\n",
    "print(\"Semantic Chunking splits by meaning/topic changes\")\n",
    "print(\"Requires embeddings model (OpenAI or local)\")\n",
    "print(\"Most intelligent but also most computationally expensive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed3f58b",
   "metadata": {},
   "source": [
    "## Comparing Different Chunk Sizes\n",
    "\n",
    "Let's see how different chunk sizes affect the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a608b8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing different chunk sizes:\n",
      "\n",
      "Chunk Size      Total Chunks    Avg Chunk      \n",
      "---------------------------------------------\n",
      "500             125             428            \n",
      "1000            67              793            \n",
      "2000            30              1578           \n",
      "\n",
      "üí° Notice: Smaller chunks = More chunks (better precision)\n",
      "üí° Larger chunks = Fewer chunks (more context per chunk)\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Test different chunk sizes\n",
    "chunk_sizes = [500, 1000, 2000]\n",
    "\n",
    "print(\"Comparing different chunk sizes:\\n\")\n",
    "print(f\"{'Chunk Size':<15} {'Total Chunks':<15} {'Avg Chunk':<15}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for size in chunk_sizes:\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=size,\n",
    "        chunk_overlap=int(size * 0.2)  # 20% overlap\n",
    "    )\n",
    "    test_chunks = splitter.split_documents(pages1)\n",
    "    avg_size = sum(len(c.page_content) for c in test_chunks) / len(test_chunks)\n",
    "    \n",
    "    print(f\"{size:<15} {len(test_chunks):<15} {avg_size:<15.0f}\")\n",
    "\n",
    "print(\"\\nüí° Notice: Smaller chunks = More chunks (better precision)\")\n",
    "print(\"üí° Larger chunks = Fewer chunks (more context per chunk)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5809ce",
   "metadata": {},
   "source": [
    "## Chunk Overlap Visualization\n",
    "\n",
    "Let's see how overlap works in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d503291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing overlap between consecutive chunks:\n",
      "\n",
      "--- Chunk 1 ends with: ---\n",
      "nt \n",
      " \n",
      " \n",
      " \n",
      "This is a pre-print version of our paper that will be published in Qualitative Research \n",
      "Journal (https://doi.org/10.1108/QRJ -12-2018-0012)\n",
      "\n",
      "--- Chunk 2 starts with: ---\n",
      "2 \n",
      "Purpose \n",
      "Qualitative research has gained in importance in the social sciences. General knowledge about \n",
      "qualitative data analysis, how to code qual\n",
      "\n",
      "============================================================\n",
      "\n",
      "--- Chunk 2 ends with: ---\n",
      "rticle \n",
      "offers researchers who are new to qualitative research a thorough yet practical introduction to \n",
      "the vocabulary and craft of coding. \n",
      " \n",
      "Design\n",
      "\n",
      "--- Chunk 3 starts with: ---\n",
      "the vocabulary and craft of coding. \n",
      " \n",
      "Design \n",
      "Having pooled our experience in coding qualitative material and teaching students how to \n",
      "code, in this\n",
      "\n",
      "============================================================\n",
      "\n",
      "üëÜ Notice how chunks share some text to maintain context!\n"
     ]
    }
   ],
   "source": [
    "# Create splitter with overlap\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "# Get first few chunks\n",
    "overlap_chunks = splitter.split_documents(pages1[:2])\n",
    "\n",
    "print(\"Showing overlap between consecutive chunks:\\n\")\n",
    "\n",
    "# Compare first two chunks\n",
    "for i in range(min(2, len(overlap_chunks)-1)):\n",
    "    chunk1 = overlap_chunks[i].page_content\n",
    "    chunk2 = overlap_chunks[i+1].page_content\n",
    "    \n",
    "    # Find overlapping text\n",
    "    print(f\"--- Chunk {i+1} ends with: ---\")\n",
    "    print(chunk1[-150:])  # Last 150 chars of chunk 1\n",
    "    \n",
    "    print(f\"\\n--- Chunk {i+2} starts with: ---\")\n",
    "    print(chunk2[:150])  # First 150 chars of chunk 2\n",
    "    \n",
    "    print(f\"\\n{'='*60}\\n\")\n",
    "\n",
    "print(\"üëÜ Notice how chunks share some text to maintain context!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b9db0b",
   "metadata": {},
   "source": [
    "## Metadata Preservation\n",
    "\n",
    "Chunks preserve metadata from the original document (page numbers, source, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aeff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "chunks_with_metadata = splitter.split_documents(pages1)\n",
    "\n",
    "print(\"Metadata in chunks:\\n\")\n",
    "for i in range(min(5, len(chunks_with_metadata))):\n",
    "    chunk = chunks_with_metadata[i]\n",
    "    print(f\"Chunk {i+1}:\")\n",
    "    print(f\"  Source: {chunk.metadata.get('source', 'N/A')}\")\n",
    "    print(f\"  Page: {chunk.metadata.get('page', 'N/A')}\")\n",
    "    print(f\"  Length: {len(chunk.page_content)} chars\")\n",
    "    print()\n",
    "\n",
    "print(\"üí° Metadata helps track where each chunk came from!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0659ca4c",
   "metadata": {},
   "source": [
    "## Practical Example: RAG Pipeline\n",
    "\n",
    "Let's see how chunking fits into a real RAG application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eea01b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Pipeline Steps:\n",
      "\n",
      "1Ô∏è‚É£ Loading documents...\n",
      "   Loaded 27 pages\n",
      "\n",
      "2Ô∏è‚É£ Chunking documents...\n",
      "   Created 67 chunks\n",
      "\n",
      "3Ô∏è‚É£ Next steps (not executed):\n",
      "   - Create embeddings for each chunk\n",
      "   - Store in vector database (Chroma, Pinecone, etc.)\n",
      "   - When user asks question:\n",
      "     a) Convert question to embedding\n",
      "     b) Find most similar chunks\n",
      "     c) Send relevant chunks to LLM\n",
      "     d) LLM generates answer\n",
      "\n",
      "‚úÖ Chunking is Step 2 in the RAG pipeline!\n"
     ]
    }
   ],
   "source": [
    "# Complete RAG Pipeline Step-by-Step\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "print(\"RAG Pipeline Steps:\\n\")\n",
    "\n",
    "# Step 1: Load documents\n",
    "print(\"1Ô∏è‚É£ Loading documents...\")\n",
    "loader = PyPDFLoader(pdf1_path)\n",
    "documents = loader.load()\n",
    "print(f\"   Loaded {len(documents)} pages\")\n",
    "\n",
    "# Step 2: Chunk documents\n",
    "print(\"\\n2Ô∏è‚É£ Chunking documents...\")\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "chunks = splitter.split_documents(documents)\n",
    "print(f\"   Created {len(chunks)} chunks\")\n",
    "\n",
    "# Step 3: Would create embeddings (not shown - requires API key)\n",
    "print(\"\\n3Ô∏è‚É£ Next steps (not executed):\")\n",
    "print(\"   - Create embeddings for each chunk\")\n",
    "print(\"   - Store in vector database (Chroma, Pinecone, etc.)\")\n",
    "print(\"   - When user asks question:\")\n",
    "print(\"     a) Convert question to embedding\")\n",
    "print(\"     b) Find most similar chunks\")\n",
    "print(\"     c) Send relevant chunks to LLM\")\n",
    "print(\"     d) LLM generates answer\")\n",
    "\n",
    "print(\"\\n‚úÖ Chunking is Step 2 in the RAG pipeline!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e5b1cb",
   "metadata": {},
   "source": [
    "## Best Practices for Chunking\n",
    "\n",
    "### 1. Chunk Size Guidelines\n",
    "\n",
    "| Document Type | Recommended Chunk Size | Reason |\n",
    "|--------------|----------------------|--------|\n",
    "| **Research Papers** | 1000-1500 | Preserve paragraph context |\n",
    "| **Books** | 1500-2000 | Longer narrative context |\n",
    "| **News Articles** | 500-1000 | Shorter, focused content |\n",
    "| **Technical Docs** | 800-1200 | Balance detail and context |\n",
    "| **Social Media** | 200-500 | Short, standalone posts |\n",
    "\n",
    "### 2. Overlap Rules\n",
    "- **General rule:** 10-20% of chunk size\n",
    "- **More overlap** = Better context continuity but more storage\n",
    "- **Less overlap** = Less storage but might lose context\n",
    "\n",
    "### 3. Choosing a Splitter\n",
    "\n",
    "```python\n",
    "# For most cases (‚≠ê Recommended)\n",
    "RecursiveCharacterTextSplitter  # Intelligent, preserves structure\n",
    "\n",
    "# For simple documents\n",
    "CharacterTextSplitter  # Fast, basic\n",
    "\n",
    "# For precise token counting\n",
    "TokenTextSplitter  # Matches LLM token limits exactly\n",
    "\n",
    "# For maximum intelligence (advanced)\n",
    "SemanticChunker  # Splits by meaning (requires embeddings)\n",
    "```\n",
    "\n",
    "### 4. Testing Your Chunks\n",
    "\n",
    "Always check:\n",
    "1. Are chunks too small? (losing context)\n",
    "2. Are chunks too large? (exceeding token limits)\n",
    "3. Do chunks split mid-sentence? (bad)\n",
    "4. Do chunks preserve meaning? (important)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9526da",
   "metadata": {},
   "source": [
    "## Advanced: Custom Splitting Logic\n",
    "\n",
    "You can create custom splitters for specific needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0efff85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Custom splitter for academic papers (preserve sections)\n",
    "academic_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=300,\n",
    "    separators=[\n",
    "        \"\\n## \",      # Markdown headers\n",
    "        \"\\n### \",     # Sub-headers\n",
    "        \"\\n\\n\",       # Paragraphs\n",
    "        \"\\n\",         # Lines\n",
    "        \". \",         # Sentences\n",
    "        \" \",          # Words\n",
    "        \"\"            # Characters\n",
    "    ]\n",
    ")\n",
    "\n",
    "academic_chunks = academic_splitter.split_documents(pages1)\n",
    "print(f\"Academic splitter created {len(academic_chunks)} chunks\")\n",
    "print(\"\\nThis splitter tries to keep sections and subsections together!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50fb9b3",
   "metadata": {},
   "source": [
    "## Summary: The Complete Chunking Process\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Why Chunk?**\n",
    "   - LLM token limits\n",
    "   - Better retrieval\n",
    "   - Cost efficiency\n",
    "\n",
    "2. **Key Parameters:**\n",
    "   - `chunk_size`: Max characters per chunk (500-2000)\n",
    "   - `chunk_overlap`: Shared characters (10-20% of chunk_size)\n",
    "   - `separators`: How to split (paragraphs ‚Üí sentences ‚Üí words)\n",
    "\n",
    "3. **Splitter Types:**\n",
    "   - **RecursiveCharacterTextSplitter** ‚≠ê Best for most cases\n",
    "   - **CharacterTextSplitter** - Simple splitting\n",
    "   - **TokenTextSplitter** - Token-based (for LLMs)\n",
    "   - **SemanticChunker** - AI-powered (advanced)\n",
    "\n",
    "4. **Best Practices:**\n",
    "   - Start with chunk_size=1000, overlap=200\n",
    "   - Test with your specific documents\n",
    "   - Check chunk quality manually\n",
    "   - Preserve metadata\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "After chunking, you would:\n",
    "1. Create embeddings for each chunk\n",
    "2. Store in a vector database\n",
    "3. Use for retrieval in RAG applications\n",
    "\n",
    "**Remember:** Good chunking = Better RAG performance! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG Langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
